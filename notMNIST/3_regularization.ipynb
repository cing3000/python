{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 326.427460\n",
      "Minibatch accuracy: 9.0%\n",
      "Validation accuracy: 31.5%\n",
      "Minibatch loss at step 500: 65.923042\n",
      "Minibatch accuracy: 77.0%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 1000: 37.085938\n",
      "Minibatch accuracy: 78.5%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 1500: 22.250946\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2000: 13.808878\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 2500: 8.489998\n",
      "Minibatch accuracy: 84.0%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 3000: 5.262975\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 3500: 3.332768\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 4000: 2.173202\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 4500: 1.449038\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 5000: 1.157944\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Test accuracy: 93.5%\n",
      "Elasped: 25.541553735733032\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "\n",
    "batch_size = 256\n",
    "learning_rate = 0.5\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "num_steps = 5001\n",
    "weights_penalty = 0.001\n",
    "\n",
    "def multilayer_model(x, weights, biases):\n",
    "    # Hidden layer\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['l1']), biases['l1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    #layer_2 = tf.add(tf.matmul(layer_1, weights['l2']), biases['l2'])\n",
    "    #layer_2 = tf.nn.relu(layer_2)\n",
    "    y = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n",
    "    return y\n",
    "    \n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables.\n",
    "    weights = {\n",
    "        'l1': tf.Variable(tf.random_normal([image_size * image_size, n_hidden_1])),\n",
    "        'l2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, num_labels]))\n",
    "    }\n",
    "    biases = {\n",
    "        'l1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'l2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([num_labels]))\n",
    "    }\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = multilayer_model(tf_train_dataset, weights, biases)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "                          + weights_penalty*tf.nn.l2_loss(weights['l1'])\n",
    "                          #+ weights_penalty*tf.nn.l2_loss(weights['l2'])\n",
    "                         + weights_penalty*tf.nn.l2_loss(weights['out']))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(multilayer_model(tf_valid_dataset, weights, biases))\n",
    "    test_prediction = tf.nn.softmax(multilayer_model(tf_test_dataset, weights, biases))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('Elasped: %s' % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 359.431946\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 34.6%\n",
      "Minibatch loss at step 500: 63.832409\n",
      "Minibatch accuracy: 72.3%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 1000: 37.341629\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 1500: 22.336929\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2000: 13.773964\n",
      "Minibatch accuracy: 78.5%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2500: 8.532104\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 5.287638\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3500: 3.334022\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 4000: 2.221980\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 4500: 1.477416\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 5000: 1.161316\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 5500: 0.879503\n",
      "Minibatch accuracy: 87.1%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 6000: 0.692871\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 6500: 0.645129\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 7000: 0.610362\n",
      "Minibatch accuracy: 86.3%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 7500: 0.493819\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 8000: 0.628657\n",
      "Minibatch accuracy: 84.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 8500: 0.487570\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 9000: 0.464506\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 9500: 0.516508\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 10000: 0.469702\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.3%\n",
      "Test accuracy: 94.1%\n",
      "Elasped: 55.60856032371521\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "\n",
    "batch_size = 256\n",
    "learning_rate = 0.5\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "num_steps = 10001\n",
    "weights_penalty = 0.001\n",
    "dropout_rate = 0.8\n",
    "\n",
    "def train_model(x, weights, biases):\n",
    "    # Hidden layer\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['l1']), biases['l1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_1 = tf.nn.dropout(layer_1, dropout_rate)\n",
    "    #layer_2 = tf.add(tf.matmul(layer_1, weights['l2']), biases['l2'])\n",
    "    #layer_2 = tf.nn.relu(layer_2)\n",
    "    y = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n",
    "    return y\n",
    "    \n",
    "def evaluate_model(x, weights, biases):\n",
    "    # Hidden layer\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['l1']), biases['l1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    #layer_2 = tf.add(tf.matmul(layer_1, weights['l2']), biases['l2'])\n",
    "    #layer_2 = tf.nn.relu(layer_2)\n",
    "    y = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n",
    "    return y\n",
    "    \n",
    "    \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables.\n",
    "    weights = {\n",
    "        'l1': tf.Variable(tf.random_normal([image_size * image_size, n_hidden_1])),\n",
    "        'l2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, num_labels]))\n",
    "    }\n",
    "    biases = {\n",
    "        'l1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'l2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([num_labels]))\n",
    "    }\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = train_model(tf_train_dataset, weights, biases)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "                          + weights_penalty*tf.nn.l2_loss(weights['l1'])\n",
    "                          #+ weights_penalty*tf.nn.l2_loss(weights['l2'])\n",
    "                         + weights_penalty*tf.nn.l2_loss(weights['out']))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(evaluate_model(tf_valid_dataset, weights, biases))\n",
    "    test_prediction = tf.nn.softmax(evaluate_model(tf_test_dataset, weights, biases))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('Elasped: %s' % elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 122.521553\n",
      "Minibatch accuracy: 9.8%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 500: 6.669133\n",
      "Minibatch accuracy: 65.2%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 1000: 6.257179\n",
      "Minibatch accuracy: 74.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 1500: 6.111706\n",
      "Minibatch accuracy: 77.2%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 2000: 6.026469\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2500: 5.914476\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 3000: 5.878120\n",
      "Minibatch accuracy: 80.2%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 3500: 5.767183\n",
      "Minibatch accuracy: 81.4%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 4000: 5.637526\n",
      "Minibatch accuracy: 82.9%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 4500: 5.672501\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 5000: 5.614084\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 5500: 5.557950\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 6000: 5.421469\n",
      "Minibatch accuracy: 83.7%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 6500: 5.397401\n",
      "Minibatch accuracy: 83.7%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 7000: 5.316576\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 7500: 5.355596\n",
      "Minibatch accuracy: 81.7%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 8000: 5.277510\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 8500: 5.167088\n",
      "Minibatch accuracy: 84.5%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 9000: 5.143601\n",
      "Minibatch accuracy: 83.9%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 9500: 5.066239\n",
      "Minibatch accuracy: 85.7%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 10000: 5.003781\n",
      "Minibatch accuracy: 85.7%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 10500: 4.972075\n",
      "Minibatch accuracy: 85.1%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 11000: 4.913821\n",
      "Minibatch accuracy: 84.7%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 11500: 4.962717\n",
      "Minibatch accuracy: 82.7%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 12000: 4.810910\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 12500: 4.819303\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 13000: 4.717309\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 13500: 4.667278\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 14000: 4.618310\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 14500: 4.568330\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 15000: 4.564184\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 15500: 4.487813\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 16000: 4.443928\n",
      "Minibatch accuracy: 86.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 16500: 4.407680\n",
      "Minibatch accuracy: 86.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 17000: 4.367120\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 17500: 4.308056\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 18000: 4.284765\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 18500: 4.226039\n",
      "Minibatch accuracy: 87.6%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 19000: 4.164414\n",
      "Minibatch accuracy: 88.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 19500: 4.205518\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 20000: 4.137478\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 20500: 4.114872\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 21000: 4.084399\n",
      "Minibatch accuracy: 86.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 21500: 3.954590\n",
      "Minibatch accuracy: 89.3%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 22000: 3.935285\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 22500: 3.912072\n",
      "Minibatch accuracy: 88.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 23000: 3.917881\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 23500: 3.867473\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 24000: 3.773878\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 24500: 3.707621\n",
      "Minibatch accuracy: 90.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 25000: 3.705884\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 25500: 3.682264\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 26000: 3.693439\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 26500: 3.661221\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 27000: 3.599191\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 27500: 3.568041\n",
      "Minibatch accuracy: 88.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 28000: 3.484350\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 28500: 3.468966\n",
      "Minibatch accuracy: 89.0%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 29000: 3.481349\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 29500: 3.460862\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 30000: 3.386703\n",
      "Minibatch accuracy: 90.1%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 30500: 3.404350\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 31000: 3.344141\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 31500: 3.295474\n",
      "Minibatch accuracy: 89.6%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 32000: 3.262060\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 32500: 3.219202\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 33000: 3.210132\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 33500: 3.227810\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 34000: 3.173201\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 34500: 3.120553\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 35000: 3.077169\n",
      "Minibatch accuracy: 89.9%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 35500: 3.015695\n",
      "Minibatch accuracy: 90.5%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 36000: 3.025797\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 36500: 3.031275\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 37000: 2.946805\n",
      "Minibatch accuracy: 91.3%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 37500: 2.947456\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 38000: 2.917564\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 38500: 2.918348\n",
      "Minibatch accuracy: 88.5%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 39000: 2.820475\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 39500: 2.857663\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 40000: 2.770795\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 40500: 2.769368\n",
      "Minibatch accuracy: 90.5%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 41000: 2.757425\n",
      "Minibatch accuracy: 91.3%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 41500: 2.737095\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 42000: 2.700600\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 42500: 2.649240\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 43000: 2.635839\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 43500: 2.642825\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 44000: 2.621248\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 44500: 2.573647\n",
      "Minibatch accuracy: 91.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 45000: 2.537522\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 45500: 2.515697\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 46000: 2.500481\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 46500: 2.477502\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 47000: 2.429268\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 47500: 2.448547\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 48000: 2.405909\n",
      "Minibatch accuracy: 90.5%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 48500: 2.405702\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 49000: 2.347272\n",
      "Minibatch accuracy: 92.3%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 49500: 2.338198\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 50000: 2.312493\n",
      "Minibatch accuracy: 92.0%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 50500: 2.324374\n",
      "Minibatch accuracy: 89.9%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 51000: 2.243929\n",
      "Minibatch accuracy: 92.4%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 51500: 2.270469\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 52000: 2.220959\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 52500: 2.271798\n",
      "Minibatch accuracy: 89.9%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 53000: 2.176634\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 53500: 2.160930\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 54000: 2.144257\n",
      "Minibatch accuracy: 92.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 54500: 2.111141\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 55000: 2.128466\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 55500: 2.049530\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 56000: 2.096527\n",
      "Minibatch accuracy: 91.3%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 56500: 2.063057\n",
      "Minibatch accuracy: 92.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 57000: 2.022033\n",
      "Minibatch accuracy: 92.3%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 57500: 2.030297\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 58000: 2.002463\n",
      "Minibatch accuracy: 92.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 58500: 1.957117\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 59000: 1.972289\n",
      "Minibatch accuracy: 92.3%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 59500: 1.927912\n",
      "Minibatch accuracy: 92.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 60000: 1.903214\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 60500: 1.914311\n",
      "Minibatch accuracy: 92.9%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 61000: 1.888155\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 61500: 1.875108\n",
      "Minibatch accuracy: 92.0%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 62000: 1.837696\n",
      "Minibatch accuracy: 93.6%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 62500: 1.812100\n",
      "Minibatch accuracy: 93.3%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 63000: 1.791422\n",
      "Minibatch accuracy: 93.5%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 63500: 1.785881\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 64000: 1.796661\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 64500: 1.796573\n",
      "Minibatch accuracy: 92.8%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 65000: 1.724962\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 65500: 1.699128\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 66000: 1.698270\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 66500: 1.678915\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 67000: 1.674363\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 67500: 1.699481\n",
      "Minibatch accuracy: 92.7%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 68000: 1.656896\n",
      "Minibatch accuracy: 93.6%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 68500: 1.624025\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 69000: 1.618581\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 69500: 1.588402\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 70000: 1.596716\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 70500: 1.598535\n",
      "Minibatch accuracy: 93.6%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 71000: 1.558078\n",
      "Minibatch accuracy: 93.9%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 71500: 1.572196\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 72000: 1.520892\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 72500: 1.527492\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 73000: 1.503134\n",
      "Minibatch accuracy: 94.0%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 73500: 1.488630\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 74000: 1.450346\n",
      "Minibatch accuracy: 94.8%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 74500: 1.481803\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 75000: 1.432006\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 75500: 1.432690\n",
      "Minibatch accuracy: 94.2%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 76000: 1.389917\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 76500: 1.396221\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 77000: 1.380900\n",
      "Minibatch accuracy: 94.6%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 77500: 1.391215\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 78000: 1.379661\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 78500: 1.360489\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 79000: 1.360005\n",
      "Minibatch accuracy: 94.4%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 79500: 1.336730\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 80000: 1.293744\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 80500: 1.314561\n",
      "Minibatch accuracy: 94.6%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 81000: 1.266032\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 81500: 1.272259\n",
      "Minibatch accuracy: 95.1%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 82000: 1.296418\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 82500: 1.254269\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 83000: 1.261134\n",
      "Minibatch accuracy: 94.3%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 83500: 1.224169\n",
      "Minibatch accuracy: 95.2%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 84000: 1.204293\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 84500: 1.203239\n",
      "Minibatch accuracy: 95.4%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 85000: 1.197690\n",
      "Minibatch accuracy: 95.7%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 85500: 1.171774\n",
      "Minibatch accuracy: 95.2%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 86000: 1.160739\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 86500: 1.146696\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 87000: 1.146709\n",
      "Minibatch accuracy: 95.4%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 87500: 1.163686\n",
      "Minibatch accuracy: 95.6%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 88000: 1.109592\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 88500: 1.138433\n",
      "Minibatch accuracy: 95.2%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 89000: 1.092355\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 92.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 89500: 1.092338\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 90000: 1.066044\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 90500: 1.076127\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 91000: 1.089172\n",
      "Minibatch accuracy: 95.4%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 91500: 1.081874\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 92000: 1.030706\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 92500: 1.042050\n",
      "Minibatch accuracy: 96.0%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 93000: 1.020159\n",
      "Minibatch accuracy: 96.7%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 93500: 1.042976\n",
      "Minibatch accuracy: 95.8%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 94000: 1.002438\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 94500: 1.004008\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 95000: 0.997351\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 95500: 0.973951\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 96000: 0.980868\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 96500: 0.941445\n",
      "Minibatch accuracy: 97.3%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 97000: 0.958394\n",
      "Minibatch accuracy: 96.3%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 97500: 0.946526\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 98000: 0.929106\n",
      "Minibatch accuracy: 96.4%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 98500: 0.911313\n",
      "Minibatch accuracy: 97.1%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 99000: 0.899034\n",
      "Minibatch accuracy: 97.6%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 99500: 0.902352\n",
      "Minibatch accuracy: 96.6%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 100000: 0.896733\n",
      "Minibatch accuracy: 97.8%\n",
      "Validation accuracy: 92.6%\n",
      "Test accuracy: 96.3%\n",
      "Elasped: 99434.30627655983\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmdWcg97MrOb+PxVL1+upaJKkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4b88f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "t = time.time()\n",
    "loss_series = []\n",
    "\n",
    "batch_size = 1024\n",
    "initial_learning_rate = 0.1\n",
    "decay_rate = 0.96\n",
    "n_hidden_1 = 4096\n",
    "n_hidden_2 = 2048\n",
    "n_hidden_3 = 1024\n",
    "num_steps = 100001\n",
    "weights_penalty = 0.0001\n",
    "dropout_rate = 0.5\n",
    "\n",
    "def train_model(x, weights, biases):\n",
    "    # Hidden layer\n",
    "    linear = tf.add(tf.matmul(x, weights['l1']), biases['l1'])\n",
    "    relu = tf.nn.relu(linear)\n",
    "    dropout = tf.nn.dropout(relu, dropout_rate)\n",
    "    \n",
    "    linear = tf.add(tf.matmul(dropout, weights['l2']), biases['l2'])\n",
    "    relu = tf.nn.relu(linear)\n",
    "    dropout = tf.nn.dropout(relu, dropout_rate)\n",
    "    \n",
    "    linear = tf.add(tf.matmul(dropout, weights['l3']), biases['l3'])\n",
    "    relu = tf.nn.relu(linear)\n",
    "    dropout = tf.nn.dropout(relu, dropout_rate)\n",
    "    \n",
    "    y = tf.add(tf.matmul(dropout, weights['out']), biases['out'])\n",
    "    return y\n",
    "    \n",
    "def evaluate_model(x, weights, biases):\n",
    "    # Hidden layer\n",
    "    linear = tf.add(tf.matmul(x, weights['l1']), biases['l1'])\n",
    "    relu = tf.nn.relu(linear)\n",
    "    \n",
    "    linear = tf.add(tf.matmul(relu, weights['l2']), biases['l2'])\n",
    "    relu = tf.nn.relu(linear)\n",
    "    \n",
    "    linear = tf.add(tf.matmul(relu, weights['l3']), biases['l3'])\n",
    "    relu = tf.nn.relu(linear)\n",
    "    \n",
    "    y = tf.add(tf.matmul(relu, weights['out']), biases['out'])\n",
    "    return y\n",
    "    \n",
    "    \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables.\n",
    "    weights = {\n",
    "        'l1': tf.Variable(tf.truncated_normal([image_size * image_size, n_hidden_1], stddev=0.1)),\n",
    "        'l2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], stddev=0.1)),\n",
    "        'l3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3], stddev=0.1)),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_3, num_labels], stddev=0.1))\n",
    "    }\n",
    "    biases = {\n",
    "        'l1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'l2': tf.Variable(tf.zeros([n_hidden_2])),\n",
    "        'l3': tf.Variable(tf.zeros([n_hidden_3])),\n",
    "        'out': tf.Variable(tf.zeros([num_labels]))\n",
    "    }\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = train_model(tf_train_dataset, weights, biases)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "                          + weights_penalty*tf.nn.l2_loss(weights['l1'])\n",
    "                          + weights_penalty*tf.nn.l2_loss(weights['l2'])\n",
    "                          + weights_penalty*tf.nn.l2_loss(weights['l3'])\n",
    "                          + weights_penalty*tf.nn.l2_loss(weights['out']))\n",
    "    \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, num_steps, decay_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(evaluate_model(tf_valid_dataset, weights, biases))\n",
    "    test_prediction = tf.nn.softmax(evaluate_model(tf_test_dataset, weights, biases))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            loss_series.append(l)\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('Elasped: %s' % elapsed)\n",
    "\n",
    "plt.plot(loss_series)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
