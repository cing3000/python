{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.913177\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.104410\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 58.5%\n",
      "Minibatch loss at step 100: 1.266695\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 150: 1.200520\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 200: 1.018956\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 250: 0.367287\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 300: 1.021968\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 350: 0.467771\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 400: 0.106202\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 450: 0.634112\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 500: 0.741699\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 550: 0.392411\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 600: 0.282339\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 650: 0.159645\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 700: 0.662776\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 750: 0.932180\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 800: 1.390816\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 850: 0.710173\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 900: 0.536027\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 950: 0.712533\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 1000: 0.834350\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.4%\n",
      "Test accuracy: 89.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.700264\n",
      "Minibatch accuracy: 17.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 500: 0.731014\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1000: 0.703233\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1500: 0.544789\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 2000: 0.338990\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 2500: 0.413246\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 3000: 0.503424\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 3500: 0.576128\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 4000: 0.431490\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 4500: 0.797310\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 5000: 0.544933\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 5500: 0.294753\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 6000: 0.561902\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 6500: 0.281704\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 7000: 0.454887\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 7500: 0.443684\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 8000: 0.359458\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 8500: 0.369715\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 9000: 0.383638\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 9500: 0.437601\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 10000: 0.442221\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 10500: 0.463540\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 11000: 0.359761\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 11500: 0.316314\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 12000: 0.660389\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 12500: 0.512861\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 13000: 0.374775\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 13500: 0.433768\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 14000: 0.224540\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 14500: 0.253646\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 15000: 0.424022\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 15500: 0.338670\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 16000: 0.445659\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 16500: 0.406298\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 17000: 0.360585\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 17500: 0.634357\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 18000: 0.226536\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 18500: 0.353158\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 19000: 0.469372\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 19500: 0.293914\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 20000: 0.323088\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 20500: 0.474686\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 21000: 0.449720\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 21500: 0.375181\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 22000: 0.452740\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 22500: 0.323410\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 23000: 0.398569\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 23500: 0.378002\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 24000: 0.607788\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 24500: 0.353907\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 25000: 0.554487\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 25500: 0.465690\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 26000: 0.543475\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 26500: 0.351219\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 27000: 0.295214\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 27500: 0.687973\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 28000: 0.327567\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 28500: 0.479062\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 29000: 0.615922\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 29500: 0.277619\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 30000: 0.336831\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 30500: 0.278779\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 31000: 0.486903\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 31500: 0.352525\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 32000: 0.420470\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 32500: 0.259830\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 33000: 0.345713\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 33500: 0.326922\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 34000: 0.348685\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 34500: 0.379603\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 35000: 0.412699\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 35500: 0.260059\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 36000: 0.268052\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 36500: 0.399362\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 37000: 0.212366\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 37500: 0.406189\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 38000: 0.557085\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 38500: 0.444469\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 39000: 0.158501\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 39500: 0.215919\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 40000: 0.317446\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 40500: 0.184532\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 41000: 0.507430\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 41500: 0.403403\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 42000: 0.215064\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 42500: 0.469857\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 43000: 0.451192\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 43500: 0.389164\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 44000: 0.218446\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 44500: 0.369652\n",
      "Minibatch accuracy: 90.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 45000: 0.213568\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 45500: 0.328850\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 46000: 0.317242\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 46500: 0.444307\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 47000: 0.553545\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 47500: 0.409577\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 48000: 0.309722\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 48500: 0.367176\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 49000: 0.744951\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 49500: 0.355329\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 50000: 0.394264\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 50500: 0.361509\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 51000: 0.381499\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 51500: 0.307621\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 52000: 0.394288\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 52500: 0.316313\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 53000: 0.242539\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 53500: 0.435115\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 54000: 0.328690\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 54500: 0.257601\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 55000: 0.208107\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 55500: 0.342158\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 56000: 0.213996\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 56500: 0.289322\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 57000: 0.394471\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 57500: 0.341511\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 58000: 0.387296\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 58500: 0.449654\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 59000: 0.332025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 59500: 0.322618\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 60000: 0.323825\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 60500: 0.731871\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 61000: 0.239790\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 61500: 0.538625\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 62000: 0.410800\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 62500: 0.312882\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 63000: 0.279671\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 63500: 0.229632\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 64000: 0.208747\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 64500: 0.392344\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 65000: 0.249496\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 65500: 0.744700\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 66000: 0.446113\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 66500: 0.229640\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 67000: 0.230295\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 67500: 0.296448\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 68000: 0.366414\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 68500: 0.220051\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 69000: 0.185794\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 69500: 0.249498\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 70000: 0.400168\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 70500: 0.277695\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 71000: 0.346592\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 71500: 0.239189\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 72000: 0.354972\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 72500: 0.427283\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 73000: 0.365383\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 73500: 0.233299\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 74000: 0.189791\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 74500: 0.344469\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 75000: 0.350212\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 75500: 0.291347\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 76000: 0.264003\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 76500: 0.482639\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 77000: 0.255830\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 77500: 0.218181\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 78000: 0.427080\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 78500: 0.535059\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 79000: 0.188853\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 79500: 0.255453\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 80000: 0.409008\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 80500: 0.359899\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 81000: 0.401026\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 81500: 0.220737\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 82000: 0.120923\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 82500: 0.413293\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 83000: 0.385644\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 83500: 0.347899\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 84000: 0.406221\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 84500: 0.242282\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 85000: 0.217366\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 85500: 0.466267\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 86000: 0.185840\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 86500: 0.421028\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 87000: 0.283192\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 87500: 0.459893\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 88000: 0.313628\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 88500: 0.309070\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 89000: 0.317275\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 89500: 0.566146\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 90000: 0.491114\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 90500: 0.186469\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 91000: 0.610046\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 91500: 0.333986\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 92000: 0.297454\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 92500: 0.319086\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 93000: 0.395375\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 93500: 0.519242\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 94000: 0.591796\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 94500: 0.361407\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 95000: 0.376087\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 95500: 0.401760\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 96000: 0.461005\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 96500: 0.479478\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 97000: 0.229712\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 97500: 0.328651\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 98000: 0.371612\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 98500: 0.354896\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 99000: 0.345681\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 99500: 0.333519\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 100000: 0.138755\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Test accuracy: 96.3%\n",
      "Elasped: 4844.65750002861\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x37f59b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 64     # 128\n",
    "num_hidden = 64\n",
    "num_steps = 100001\n",
    "decay_rate = 0.98               # 0.96\n",
    "initial_learning_rate = 0.1     # 0.5\n",
    "\n",
    "# Regularization\n",
    "keep_prob = 0.5                 # dropout\n",
    "weights_penalty=0.0001          # 0.001\n",
    "\n",
    "# Convolutional hyperparameters\n",
    "patch_size = 5       # 5\n",
    "padding = 'SAME'\n",
    "depth = 16           # 32\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "t = time.time()\n",
    "loss_series = []\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "    #layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "    #        [(image_size - 3*patch_size + 3) // 4 * (image_size - 3*patch_size + 3) // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "            [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.zeros([num_hidden]))\n",
    "    \n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "    # Model.\n",
    "    def training_model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding)\n",
    "        pooling = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding)\n",
    "        hidden = tf.nn.relu(pooling + layer1_biases)\n",
    "        dropout = tf.nn.dropout(hidden, keep_prob)\n",
    "\n",
    "        conv = tf.nn.conv2d(dropout, layer2_weights, [1, 1, 1, 1], padding)\n",
    "        pooling = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding)\n",
    "        hidden = tf.nn.relu(pooling + layer2_biases)\n",
    "        dropout = tf.nn.dropout(hidden, keep_prob)\n",
    "\n",
    "        shape = dropout.get_shape().as_list()\n",
    "        reshape = tf.reshape(dropout, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "    def evluation_model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding)\n",
    "        pooling = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding)\n",
    "        hidden = tf.nn.relu(pooling + layer1_biases)\n",
    "        \n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding)\n",
    "        pooling = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding)\n",
    "        hidden = tf.nn.relu(pooling + layer2_biases)\n",
    "        \n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "    # Training computation.\n",
    "    logits = training_model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "        + weights_penalty*tf.nn.l2_loss(layer1_weights)\n",
    "        + weights_penalty*tf.nn.l2_loss(layer2_weights)\n",
    "        + weights_penalty*tf.nn.l2_loss(layer3_weights)\n",
    "        + weights_penalty*tf.nn.l2_loss(layer4_weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, num_steps, decay_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(evluation_model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(evluation_model(tf_test_dataset))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            loss_series.append(l)\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('Elasped: %s' % elapsed)\n",
    "\n",
    "plt.plot(loss_series)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 5.604386\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-16e0150bc583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         _, l, predictions = session.run(\n\u001b[1;32m--> 123\u001b[1;33m             [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Minibatch loss at step %d: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dzlu\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dzlu\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dzlu\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mc:\\users\\dzlu\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dzlu\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128     # 128\n",
    "num_hidden = 64\n",
    "num_steps = 20001\n",
    "decay_rate = 0.98               # 0.96\n",
    "initial_learning_rate = 0.1     # 0.5\n",
    "\n",
    "# Regularization\n",
    "keep_prob = 0.5                 # dropout\n",
    "weights_penalty=0.0005          # 0.001\n",
    "\n",
    "# Convolutional hyperparameters\n",
    "patch_size = 3       # 5\n",
    "padding = 'SAME'\n",
    "depth1 = 16           # 32\n",
    "depth2 = 16\n",
    "depth3 = 32\n",
    "\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, num_channels, depth1], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth1]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, depth1, depth2], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "    #layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "    #        [(image_size - 3*patch_size + 3) // 4 * (image_size - 3*patch_size + 3) // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, depth2, depth3], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth3]))\n",
    "    \n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [image_size // 7 * image_size // 7 * depth3, num_hidden], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "            [num_hidden, num_labels], stddev=0.1))\n",
    "    layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "    # Model.\n",
    "    def training_model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding)\n",
    "        pooling = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding)\n",
    "        hidden = tf.nn.relu(pooling + layer1_biases)\n",
    "        dropout = tf.nn.dropout(hidden, keep_prob)\n",
    "\n",
    "        conv = tf.nn.conv2d(dropout, layer2_weights, [1, 1, 1, 1], padding)\n",
    "        pooling = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding)\n",
    "        hidden = tf.nn.relu(pooling + layer2_biases)\n",
    "        dropout = tf.nn.dropout(hidden, keep_prob)\n",
    "        \n",
    "        conv = tf.nn.conv2d(dropout, layer3_weights, [1, 1, 1, 1], padding)\n",
    "        pooling = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding)\n",
    "        hidden = tf.nn.relu(pooling + layer3_biases)\n",
    "        dropout = tf.nn.dropout(hidden, keep_prob)\n",
    "\n",
    "        shape = dropout.get_shape().as_list()\n",
    "        reshape = tf.reshape(dropout, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "\n",
    "        return tf.matmul(hidden, layer5_weights) + layer5_biases\n",
    "\n",
    "    def evluation_model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding)\n",
    "        pooling = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding)\n",
    "        hidden = tf.nn.relu(pooling + layer1_biases)\n",
    "        \n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding)\n",
    "        pooling = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding)\n",
    "        hidden = tf.nn.relu(pooling + layer2_biases)\n",
    "        \n",
    "        conv = tf.nn.conv2d(hidden, layer3_weights, [1, 1, 1, 1], padding)\n",
    "        pooling = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding)\n",
    "        hidden = tf.nn.relu(pooling + layer3_biases)\n",
    "\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "\n",
    "        return tf.matmul(hidden, layer5_weights) + layer5_biases\n",
    "\n",
    "    # Training computation.\n",
    "    logits = training_model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "        + weights_penalty*tf.nn.l2_loss(layer1_weights)\n",
    "        + weights_penalty*tf.nn.l2_loss(layer2_weights)\n",
    "        #+ weights_penalty*tf.nn.l2_loss(layer3_weights)\n",
    "        + weights_penalty*tf.nn.l2_loss(layer4_weights)\n",
    "        + weights_penalty*tf.nn.l2_loss(layer5_weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, num_steps, decay_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(evluation_model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(evluation_model(tf_test_dataset))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('Elasped: %s' % elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMPF/wlgwHsxD5uIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa2a27b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "testa = []\n",
    "testa.append(12)\n",
    "testa.append(5)\n",
    "plt.plot(testa)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
